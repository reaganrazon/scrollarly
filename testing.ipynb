{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRIS database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"{hostname}:{port}/{namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:1972/USER\n"
     ]
    }
   ],
   "source": [
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = iris.connect(CONNECTION_STRING, username, password)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"Papers.General_Data\"\n",
    "table_definition = \"\"\"(\n",
    "    title VARCHAR(255),\n",
    "    doi VARCHAR(255) UNIQUE,\n",
    "    pub_date TEXT,\n",
    "    authorships TEXT,\n",
    "    topics TEXT,\n",
    "    cited_by INT,\n",
    "    keywords TEXT,\n",
    "    abstract TEXT,\n",
    "    titleVector VECTOR(DOUBLE, 384),\n",
    "    abstractVector VECTOR(DOUBLE, 384),\n",
    "    topicsVector VECTOR(DOUBLE, 384),\n",
    "    keywordsVector VECTOR(DOUBLE, 384)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_name = \"Papers.User_Interests\"\n",
    "interest_definition = \"\"\"(\n",
    "    user_id VARCHAR(255),\n",
    "    interests VARCHAR(255)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(f\"DROP TABLE {interest_name}\")  \n",
    "except:\n",
    "    pass\n",
    "cursor.execute(f\"CREATE TABLE {interest_name} {interest_definition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(f\"DROP TABLE {table_name}\")  \n",
    "except:\n",
    "    pass\n",
    "cursor.execute(f\"CREATE TABLE {table_name} {table_definition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    max_length = 255\n",
    "\n",
    "    if not title:\n",
    "        return None\n",
    "    \n",
    "    cleaned_title = title[:max_length]\n",
    "\n",
    "    try:\n",
    "        return cleaned_title.encode(\"utf-8\").decode(\"utf-8\") \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Invalid characters in title: {title}\")\n",
    "        return None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_doi_unique(cursor, doi):\n",
    "    \"\"\"Check if DOI already exists in the database\"\"\"\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM Papers.GeneralData WHERE doi = ?\", (doi,))\n",
    "    count = cursor.fetchone()[0]\n",
    "    return count == 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reaganrazon/Documents/Treehacks25/hackathon-2024/iris-env/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PANDAS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_embed(filename, table_name, conn):\n",
    "    \"\"\"\n",
    "    Reads JSON, processes embeddings, and uploads data into a SQL table.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: str (path to JSON file)\n",
    "    - table_name: str (SQL table name)\n",
    "    - conn: Database connection object\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        papers = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(papers)\n",
    "\n",
    "    # Clean and process columns\n",
    "    df[\"Title\"] = df[\"Title\"].fillna(\"\").apply(str)\n",
    "    df[\"DOI\"] = df[\"DOI\"].fillna(\"\").apply(str)\n",
    "    df[\"Date\"] = df[\"Date\"]\n",
    "    df[\"Authorships\"] = df[\"Authorships\"].apply(lambda x: \"; \".join(x) if isinstance(x, list) else \"\")\n",
    "    df[\"Topics\"] = df[\"Topics\"].apply(lambda x: \"; \".join(x) if isinstance(x, list) else \"\")\n",
    "    df[\"Cited by\"] = df[\"Cited by\"].fillna(0).astype(int)\n",
    "    df[\"Keywords\"] = df[\"Keywords\"].apply(lambda x: \"; \".join(kw[\"display_name\"] for kw in x) if isinstance(x, list) else \"\")\n",
    "    df[\"Abstract\"] = df[\"Abstract\"].fillna(\"\").apply(str)\n",
    "\n",
    "    # Generate embeddings for each text field\n",
    "    print(\"Generating embeddings... This may take some time.\")\n",
    "    # df[\"TitleVector\"] = df[\"Title\"].apply(generate_embedding)\n",
    "    # df[\"AbstractVector\"] = df[\"Abstract\"].apply(generate_embedding)\n",
    "    # df[\"TopicsVector\"] = df[\"Topics\"].apply(generate_embedding)\n",
    "    # df[\"KeywordsVector\"] = df[\"Keywords\"].apply(generate_embedding)\n",
    "\n",
    "    df[\"TitleVector\"] = model.encode(df[\"Title\"].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True).tolist()\n",
    "    df[\"AbstractVector\"] = model.encode(df[\"Abstract\"].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True).tolist()\n",
    "    df[\"TopicsVector\"] = model.encode(df[\"Topics\"].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True).tolist()\n",
    "    df[\"KeywordsVector\"] = model.encode(df[\"Keywords\"].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True).tolist()\n",
    "\n",
    "    # Define SQL Insert Query (uses parameterized queries for security)\n",
    "    sql = f\"\"\"INSERT INTO {table_name} (title, doi, pub_date, authorships, topics, cited_by, keywords, abstract,titleVector,abstractVector,topicsVector,keywordsVector) \n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?,TO_VECTOR(?),TO_VECTOR(?),TO_VECTOR(?),TO_VECTOR(?))\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert DataFrame to List of Tuples for Bulk Insert\n",
    "    data = df[[\"Title\", \"DOI\", \"Date\", \"Authorships\", \"Topics\", \"Cited by\", \"Keywords\", \"Abstract\", \n",
    "               \"TitleVector\", \"AbstractVector\", \"TopicsVector\", \"KeywordsVector\"]].values.tolist()\n",
    "    \n",
    "    data = [\n",
    "    (\n",
    "        row['Title'], \n",
    "        row['DOI'], \n",
    "        row['Date'], \n",
    "        row['Authorships'], \n",
    "        row['Topics'], \n",
    "        row['Cited by'], \n",
    "        row['Keywords'], \n",
    "        row['Abstract'], \n",
    "        str(row['TitleVector']),\n",
    "        str(row['AbstractVector']),\n",
    "        str(row['TopicsVector']),\n",
    "        str(row['KeywordsVector'])\n",
    "    )\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "    \n",
    "\n",
    "    # Insert into database using executemany (faster than looping)\n",
    "    cursor.executemany(sql, data)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Successfully added {len(df)} entries from {filename} in {end_time - start_time:.2f} seconds.\")\n",
    "    # return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings... This may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 40/40 [00:17<00:00,  2.31it/s]\n",
      "Batches: 100%|██████████| 40/40 [03:16<00:00,  4.90s/it]\n",
      "Batches: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:12<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 10000 entries from thousand_papers4.json in 46.57 seconds.\n",
      "Generating embeddings... This may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s]\n",
      "Batches: 100%|██████████| 40/40 [04:30<00:00,  6.77s/it]\n",
      "Batches: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:14<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 10000 entries from thousand_papers5.json in 43.09 seconds.\n",
      "Generating embeddings... This may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 40/40 [00:19<00:00,  2.10it/s]\n",
      "Batches: 100%|██████████| 40/40 [04:30<00:00,  6.77s/it]\n",
      "Batches: 100%|██████████| 40/40 [00:19<00:00,  2.07it/s]\n",
      "Batches: 100%|██████████| 40/40 [00:14<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 10000 entries from thousand_papers6.json in 45.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"thousand_papers4.json\",\"thousand_papers5.json\",\"thousand_papers6.json\"]\n",
    "\n",
    "datasets2 = [\"filtered_paper_MAIN.json\",\"filtered_paper_MAIN2.json\",\"filtered_paper_MAIN3.json\",\"filtered_paper_MAIN4.json\",\"filtered_paper_MAIN5.json\", \"filtered_paper2.json\", \"filtered_papers_paginated_2.json\",'filtered_papers_paginated_3.json',\"filtered_papers_paginated_4.json\",\"filtered_papers.json\",\"filtered_papers_paginated.json\",\"filtered_papers_paginated_7.json\",\"filtered_papers_paginated_5.json\", \"filtered_papers_paginated_6.json\"]\n",
    "\n",
    "for d in datasets:\n",
    "    upload_and_embed(d, table_name,conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in table: 32107\n"
     ]
    }
   ],
   "source": [
    "sql = f\"\"\" SELECT COUNT(*) FROM {table_name} \"\"\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "row_count = cursor.fetchone()[0]\n",
    "print(f\"Total rows in table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchPhrase = \"ballet\"\n",
    "searchVector = model.encode(searchPhrase, normalize_embeddings=True).tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The Application of 'Improvisational Accompaniment' in Piano Textbooks for Music Majors in Chinese Higher Education\", 'Educational Reforms and Innovations; Educational Technology and Pedagogy; Diverse Music Education Insights', 'This research evaluates the pedagogical application of \"Improvisational Accompaniment\" in piano textbooks within Chinese higher music education, highlighting its significance in enhancing students\\' practical skills and professional prospects. The study examines the integration of foundational piano training with improvisation, the promotion of creative teaching methods, and the unique educational features of Chinese textbooks. Empirical analysis presents a multifaceted teaching approach, including collective instruction and targeted practice, to bolster students\\' improvisational proficiency. The findings offer insights for global music educators on syllabus development and innovative teaching strategies.', '.12960514098709224883')\n",
      "('Physical Activity and Public Health in Older Adults', 'Physical Activity and Health; Cardiovascular and exercise physiology; Cardiac Health and Mental Health', \"Objective: To issue a recommendation on the types and amounts of physical activity needed to improve and maintain health in older adults. Participants: A panel of scientists with expertise in public health, behavioral science, epidemiology, exercise science, medicine, and gerontology. Evidence: The expert panel reviewed existing consensus statements and relevant evidence from primary research articles and reviews of the literature. Process: After drafting a recommendation for the older adult population and reviewing drafts of the Updated Recommendation from the American College of Sports Medicine (ACSM) and the American Heart Association (AHA) for Adults, the panel issued a final recommendation on physical activity for older adults. Summary: The recommendation for older adults is similar to the updated ACSM/AHA recommendation for adults, but has several important differences including: the recommended intensity of aerobic activity takes into account the older adult's aerobic fitness; activities that maintain or increase flexibility are recommended; and balance exercises are recommended for older adults at risk of falls. In addition, older adults should have an activity plan for achieving recommended physical activity that integrates preventive and therapeutic recommendations. The promotion of physical activity in older adults should emphasize moderate-intensity aerobic activity, muscle-strengthening activity, reducing sedentary behavior, and risk management.\", '.12960514098709224883')\n",
      "('Research on the Reform of College Physical Education Based on the Concept of Lifelong Physical Education', 'Educational Technology and Pedagogy; Educational Reforms and Innovations; Diverse Approaches in Healthcare and Education Studies', \"The concept of lifelong physical education is a teaching concept that has been put forward for more than half a century.It emphasizes that physical exercise should run through our study, work and life.So we should exercise consciously.However, college students, a huge group, are seriously lacking this concept, which deserves people's deep consideration.In view of the implementation of the concept of lifelong physical education in College Physical Education classes, this paper analyses the problems existing in College Physical Education from the perspective of the concept of lifelong physical education and puts forward four improvement measures in order to promote the reform of College Physical Education in China.\", '.12960514098709224883')\n",
      "('The Limits of Rehabilitation and Recidivism Reduction: Rethinking the Evaluation of Arts Programming in Prisons', 'Criminal Justice and Corrections Analysis; Art Therapy and Mental Health; Psychopathy, Forensic Psychiatry, Sexual Offending', 'Canadian prison-based arts and other programming are limited at best. Even the country’s Correctional Investigator, or prison-ombudsperson, has critiqued the lack of meaningful options in which prisoners can engage. Those programs that do exist tend to be focused on the logic of penal rehabilitation, with the end goal of reducing recidivism. In this article, we showcase the evaluation of a 9-week arts program in a women’s prison, the aim of which was to build community and foster artistic engagement, thus running counter to normative carceral logics.', '.12960514098709224883')\n",
      "('Syntheses of the appropriation of Vygotsky’s concept of Aesthetic Education in Brazilian art teaching (2006-2020)', 'Education Pedagogy and Practices; Education and Digital Technologies; Rural and Ethnic Education', 'Abstract The cultural-historical theory on human cognition, elaborated by the Russian researchers Vygotsky, Leontiev, and Luria between 1920 and 1930, reached psychology and education in Brazil from 1980 onwards, presenting a new dimension for the formation and performance of these professionals. Based on these assumptions, a systematic review of the Brazilian scientific literature between 2006 and 2020 was conducted, in search of articles published in areas of intersection between the psychology of education and art education, aiming to identify the circulation, reception, and appropriation of Vygotsky’s concept of aesthetic education. The results found are in line with studies that consider the concepts of “experience” and “environment” as essential for the understanding of cultural-historical theory.', '.12960514098709224883')\n",
      "('Implementasi Motion Grafis Dalam Tujuh Langkah Pelayanan Di KFC', 'Blockchain Technology in Education and Learning; Multimedia Learning Systems; Computer Science and Engineering', 'As a leading fast food restaurant in Indonesia, KFC is required to continuethe maintenance and always provide excellent service to customers. Along with the current technological advances of many learning media that utilize video media. The use of video media in conducting training and reminding of KFC employees that aimed for every employee to run Standard Operational Procedure (SOP) properly and correctly. Submission of information about the seven steps of service delivered through 2D video of Motion Graphic that uses sound effects, images, and typography so more effective and efficient. It was developed with some steps that started from analysis, design, development, implementation and evaluation (ADDIE model). By creating this motion grafic video, all the cashiers at KFC can run procedures Seven Steps Service well and correctly.', '.12960514098709224883')\n",
      "('Physiology of Sport and Exercise', 'Muscle metabolism and nutrition; Sports Performance and Training', 'Chapter 1. Structure and Function of Exercising Muscle Chapter 2. Fuel for Exercise: Bioenergetics and Muscle Metabolism Substrates Chapter 3. Neural Control of Exercising Muscle Chapter 4. Hormonal Control During Exercise Chapter 5. Energy Expenditure and Fatigue Chapter 6. The Cardiovascular System and Its Control Chapter 7. The Respiratory System and Its Regulation Chapter 8. Cardiorespiratory Responses to Acute Exercise Chapter 9. Principles of Exercise Training.', '.12960514098709224883')\n",
      "('Physical Activity/Exercise and Diabetes: A Position Statement of the American Diabetes Association', 'Diabetes Management and Research; Pancreatic function and diabetes; Diabetes Treatment and Management', 'The adoption and maintenance of physical activity are critical foci for blood glucose management and overall health in individuals with diabetes and prediabetes. Recommendations and precautions vary depending on individual characteristics and health status. In this Position Statement, we provide a clinically oriented review and evidence-based recommendations regarding physical activity and exercise in people with type 1 diabetes, type 2 diabetes, gestational diabetes mellitus, and prediabetes.\\n\\nPhysical activity includes all movement that increases energy use, whereas exercise is planned, structured physical activity. Exercise improves blood glucose control in type 2 diabetes, reduces cardiovascular risk factors, contributes to weight loss, and improves well-being (1,2). Regular exercise may prevent or delay type 2 diabetes development (3). Regular exercise also has considerable health benefits for people with type 1 diabetes (e.g., improved cardiovascular fitness, muscle strength, insulin sensitivity, etc.) (4). The challenges related to blood glucose management vary with diabetes type, activity type, and presence of diabetes-related complications (5,6). Physical activity and exercise recommendations, therefore, should be tailored to meet the specific needs of each individual.\\n\\nPhysical activity recommendations and precautions may vary by diabetes type. The primary types of diabetes are type 1 and type 2. Type 1 diabetes (5%–10% of cases) results from cellular-mediated autoimmune destruction of the pancreatic β-cells, producing insulin deficiency (7). Although it can occur at any age, β-cell destruction rates vary, typically occurring more rapidly in youth than in adults. Type 2 diabetes (90%–95% of cases) results from a progressive loss of insulin secretion, usually also with insulin resistance. Gestational diabetes mellitus occurs during pregnancy, with screening typically occurring at 24–28 weeks of gestation in pregnant women not previously known to have diabetes. Prediabetes is diagnosed when blood glucose levels are above the normal range but not high enough to be classified as …', '.12960514098709224883')\n",
      "('A Database and Evaluation Methodology for Optical Flow', 'Advanced Vision and Imaging; Advanced Image Processing Techniques; Image Enhancement Techniques', 'The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/ . Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.', '.12960514098709224883')\n",
      "('My Hobby, Chorus', 'Race, History, and American Society; Theater, Performance, and Music History; Music History and Culture', 'N/A', '.12960514098709224883')\n"
     ]
    }
   ],
   "source": [
    "sql = f\"\"\"\n",
    "    SELECT TOP ? title, topics, abstract, \n",
    "        VECTOR_DOT_PRODUCT(abstractVector, TO_VECTOR(?)) * 0.5 +\n",
    "        VECTOR_DOT_PRODUCT(titleVector, TO_VECTOR(?)) * 0.3 +\n",
    "        VECTOR_DOT_PRODUCT(keywordsVector, TO_VECTOR(?)) * 0.2 AS relevance_score\n",
    "    FROM {table_name}\n",
    "    ORDER BY relevance_score DESC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "numberOfResults = 10\n",
    "\n",
    "cursor.execute(sql, [numberOfResults, str(searchVector), str(searchVector), str(searchVector)])\n",
    "\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    connection_string = \"localhost:1972/USER\"\n",
    "    username = \"demo\"\n",
    "    password = \"demo\"\n",
    "\n",
    "    context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
    "    context.verify_mode=ssl.CERT_REQUIRED\n",
    "    context.check_hostname = False\n",
    "    context.load_verify_locations(\"c:/InterSystems/Certs/isc-cert.pem\")\n",
    "\n",
    "    connection = iris.connect(connection_string, username, password)\n",
    "\n",
    "    # when finished, use the line below to close the connection\n",
    "    # connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Query\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "app = FastAPI()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "connection_string = \"localhost:1972/USER\"\n",
    "username = \"demo\"\n",
    "password = \"demo\"\n",
    "\n",
    "conn = iris.connect(connection_string, username, password)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "@app.get(\"/papers\")\n",
    "def get_papers(\n",
    "    searchPhrase: str,\n",
    "    page: int = Query(1, alias=\"page\"), \n",
    "    per_page: int = Query(6, alias=\"per_page\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch papers with semantic search and pagination.\n",
    "    \"\"\"\n",
    "    searchVector = model.encode(searchPhrase, normalize_embeddings=True).tolist()\n",
    "\n",
    "    offset = (page - 1) * per_page\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT title, topics, abstract, \n",
    "            VECTOR_DOT_PRODUCT(abstractVector, TO_VECTOR(?)) * 0.5 +\n",
    "            VECTOR_DOT_PRODUCT(titleVector, TO_VECTOR(?)) * 0.3 +\n",
    "            VECTOR_DOT_PRODUCT(keywordsVector, TO_VECTOR(?)) * 0.2 AS relevance_score\n",
    "        FROM {table_name}\n",
    "        ORDER BY relevance_score DESC\n",
    "        LIMIT {per_page} OFFSET {offset}\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(sql, [str(searchVector), str(searchVector), str(searchVector)])\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    return {\n",
    "        \"papers\": [\n",
    "            {\n",
    "                \"title\": row[0],\n",
    "                \"topics\": row[1],\n",
    "                \"abstract\": row[2],\n",
    "            }\n",
    "            for row in results\n",
    "        ],\n",
    "        \"nextPage\": page + 1 if len(results) == per_page else None,\n",
    "        \"hasMore\": len(results) == per_page\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
